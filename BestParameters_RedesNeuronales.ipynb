{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from numpy import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from numpy import matlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('ProyectoFA/Diabetic_Retinopathy/MachineLearning_Models/Data_PCA/X_features_2.csv',delimiter=',')\n",
    "Y = np.genfromtxt('ProyectoFA/Diabetic_Retinopathy/MachineLearning_Models/Data_PCA/Y_labels_2.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño X:  (14108, 100)\n",
      "Tamaño Y:  (14108,)\n"
     ]
    }
   ],
   "source": [
    "print('Tamaño X: ',X.shape)\n",
    "print('Tamaño Y: ', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redes_neuronales():    \n",
    "    Folds = 5\n",
    "    random.seed(19680801)\n",
    "    EficienciaTrain = np.zeros(Folds)\n",
    "    EficienciaVal = np.zeros(Folds)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=Folds)\n",
    "    j = 0\n",
    "    for train, test in skf.split(X, Y):\n",
    "        Xtrain = X[train,:]\n",
    "        Ytrain = Y[train]\n",
    "        Xtest = X[test,:]\n",
    "        Ytest = Y[test]\n",
    "        \n",
    "        #Se normalizan los datos\n",
    "        media = np.mean(Xtrain)\n",
    "        desvia = np.std(Xtrain)\n",
    "        Xtrain = preprocessing.scale(Xtrain)\n",
    "        Xtest = (Xtest - np.matlib.repmat(media, Xtest.shape[0], 1))/np.matlib.repmat(desvia, Xtest.shape[0], 1)\n",
    "        \n",
    "        #Llamado a la función para crear y entrenar el modelo usando los datos de entrenamiento\n",
    "        mlp = MLPClassifier(hidden_layer_sizes=(4096,512),activation = 'relu',max_iter=1000,\n",
    "                            beta_1 = 0.00001, beta_2=0.3, alpha=0.000001, batch_size=128, verbose=True)\n",
    "        mlp.fit(Xtrain,Ytrain)\n",
    "        \n",
    "        #Validación con las muestras de entrenamiento\n",
    "        Ytrain_pred = mlp.predict(Xtrain)\n",
    "\n",
    "        #Validación con las muestras de test    \n",
    "        Yest = mlp.predict(Xtest)\n",
    "           \n",
    "        #Evaluamos las predicciones del modelo con los datos de test\n",
    "        EficienciaTrain[j] = np.mean(Ytrain_pred == Ytrain)\n",
    "        EficienciaVal[j] = np.mean(Yest == Ytest)\n",
    "        j += 1\n",
    "   \n",
    "    return np.mean(EficienciaVal), np.std(EficienciaVal),np.mean(EficienciaTrain),np.std(EficienciaTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.40758234\n",
      "Iteration 2, loss = 1.30137723\n",
      "Iteration 3, loss = 1.23680477\n",
      "Iteration 4, loss = 1.14290486\n",
      "Iteration 5, loss = 1.05219702\n",
      "Iteration 6, loss = 0.94568900\n",
      "Iteration 7, loss = 0.86413717\n",
      "Iteration 8, loss = 0.77842334\n",
      "Iteration 9, loss = 0.70084354\n",
      "Iteration 10, loss = 0.62160836\n",
      "Iteration 11, loss = 0.55020171\n",
      "Iteration 12, loss = 0.49562560\n",
      "Iteration 13, loss = 0.44443548\n",
      "Iteration 14, loss = 0.38747520\n",
      "Iteration 15, loss = 0.35705166\n",
      "Iteration 16, loss = 0.30778607\n",
      "Iteration 17, loss = 0.28513415\n",
      "Iteration 18, loss = 0.24656967\n",
      "Iteration 19, loss = 0.21944329\n",
      "Iteration 20, loss = 0.19962001\n",
      "Iteration 21, loss = 0.18087039\n",
      "Iteration 22, loss = 0.16003708\n",
      "Iteration 23, loss = 0.14632240\n",
      "Iteration 24, loss = 0.13477807\n",
      "Iteration 25, loss = 0.12496008\n",
      "Iteration 26, loss = 0.11326077\n",
      "Iteration 27, loss = 0.10299675\n",
      "Iteration 28, loss = 0.10670298\n",
      "Iteration 29, loss = 0.08613812\n",
      "Iteration 30, loss = 0.09169949\n",
      "Iteration 31, loss = 0.08198227\n",
      "Iteration 32, loss = 0.07396737\n",
      "Iteration 33, loss = 0.07948159\n",
      "Iteration 34, loss = 0.07103965\n",
      "Iteration 35, loss = 0.06617619\n",
      "Iteration 36, loss = 0.06578638\n",
      "Iteration 37, loss = 0.05976150\n",
      "Iteration 38, loss = 0.05561324\n",
      "Iteration 39, loss = 0.05853519\n",
      "Iteration 40, loss = 0.05960642\n",
      "Iteration 41, loss = 0.05832211\n",
      "Iteration 42, loss = 0.05348136\n",
      "Iteration 43, loss = 0.05390066\n",
      "Iteration 44, loss = 0.05221572\n",
      "Iteration 45, loss = 0.04904057\n",
      "Iteration 46, loss = 0.06346727\n",
      "Iteration 47, loss = 0.05090041\n",
      "Iteration 48, loss = 0.04088535\n",
      "Iteration 49, loss = 0.04354958\n",
      "Iteration 50, loss = 0.03996034\n",
      "Iteration 51, loss = 0.03769033\n",
      "Iteration 52, loss = 0.04604609\n",
      "Iteration 53, loss = 0.03860641\n",
      "Iteration 54, loss = 0.03211310\n",
      "Iteration 55, loss = 0.03925375\n",
      "Iteration 56, loss = 0.04336714\n",
      "Iteration 57, loss = 0.03550603\n",
      "Iteration 58, loss = 0.04289440\n",
      "Iteration 59, loss = 0.03993206\n",
      "Iteration 60, loss = 0.04100576\n",
      "Iteration 61, loss = 0.04158979\n",
      "Iteration 62, loss = 0.04402683\n",
      "Iteration 63, loss = 0.03293880\n",
      "Iteration 64, loss = 0.04529036\n",
      "Iteration 65, loss = 0.02964659\n",
      "Iteration 66, loss = 0.02715190\n",
      "Iteration 67, loss = 0.03868483\n",
      "Iteration 68, loss = 0.02473683\n",
      "Iteration 69, loss = 0.03127204\n",
      "Iteration 70, loss = 0.04101661\n",
      "Iteration 71, loss = 0.03274822\n",
      "Iteration 72, loss = 0.02968917\n",
      "Iteration 73, loss = 0.02918899\n",
      "Iteration 74, loss = 0.02694584\n",
      "Iteration 75, loss = 0.03330004\n",
      "Iteration 76, loss = 0.02887210\n",
      "Iteration 77, loss = 0.03561884\n",
      "Iteration 78, loss = 0.03177016\n",
      "Iteration 79, loss = 0.01997242\n",
      "Iteration 80, loss = 0.03073163\n",
      "Iteration 81, loss = 0.02794801\n",
      "Iteration 82, loss = 0.02971230\n",
      "Iteration 83, loss = 0.02249201\n",
      "Iteration 84, loss = 0.02999253\n",
      "Iteration 85, loss = 0.03524869\n",
      "Iteration 86, loss = 0.02645084\n",
      "Iteration 87, loss = 0.03104563\n",
      "Iteration 88, loss = 0.02347972\n",
      "Iteration 89, loss = 0.02974846\n",
      "Iteration 90, loss = 0.03145405\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.39690891\n",
      "Iteration 2, loss = 1.30672465\n",
      "Iteration 3, loss = 1.23332303\n",
      "Iteration 4, loss = 1.14831481\n",
      "Iteration 5, loss = 1.05991263\n",
      "Iteration 6, loss = 0.96243294\n",
      "Iteration 7, loss = 0.86853818\n",
      "Iteration 8, loss = 0.78703268\n",
      "Iteration 9, loss = 0.70043294\n",
      "Iteration 10, loss = 0.62559631\n",
      "Iteration 11, loss = 0.56071666\n",
      "Iteration 12, loss = 0.50525695\n",
      "Iteration 13, loss = 0.44278902\n",
      "Iteration 14, loss = 0.40043691\n",
      "Iteration 15, loss = 0.35165898\n",
      "Iteration 16, loss = 0.31446202\n",
      "Iteration 17, loss = 0.28225390\n",
      "Iteration 18, loss = 0.24633307\n",
      "Iteration 19, loss = 0.22360440\n",
      "Iteration 20, loss = 0.20269283\n",
      "Iteration 21, loss = 0.17419081\n",
      "Iteration 22, loss = 0.16254664\n",
      "Iteration 23, loss = 0.14616767\n",
      "Iteration 24, loss = 0.13104747\n",
      "Iteration 25, loss = 0.11798397\n",
      "Iteration 26, loss = 0.12113107\n",
      "Iteration 27, loss = 0.09825672\n",
      "Iteration 28, loss = 0.09980098\n",
      "Iteration 29, loss = 0.09751020\n",
      "Iteration 30, loss = 0.09031218\n",
      "Iteration 31, loss = 0.08738295\n",
      "Iteration 32, loss = 0.07885680\n",
      "Iteration 33, loss = 0.07741583\n",
      "Iteration 34, loss = 0.08428276\n",
      "Iteration 35, loss = 0.06506663\n",
      "Iteration 36, loss = 0.06997701\n",
      "Iteration 37, loss = 0.05843883\n",
      "Iteration 38, loss = 0.07123403\n",
      "Iteration 39, loss = 0.06215515\n",
      "Iteration 40, loss = 0.06238721\n",
      "Iteration 41, loss = 0.05296219\n",
      "Iteration 42, loss = 0.05760788\n",
      "Iteration 43, loss = 0.04302895\n",
      "Iteration 44, loss = 0.04541154\n",
      "Iteration 45, loss = 0.05726789\n",
      "Iteration 46, loss = 0.05911706\n",
      "Iteration 47, loss = 0.04231173\n",
      "Iteration 48, loss = 0.04277437\n",
      "Iteration 49, loss = 0.05712608\n",
      "Iteration 50, loss = 0.03980172\n",
      "Iteration 51, loss = 0.04039513\n",
      "Iteration 52, loss = 0.03390699\n",
      "Iteration 53, loss = 0.03606263\n",
      "Iteration 54, loss = 0.04214620\n",
      "Iteration 55, loss = 0.04244792\n",
      "Iteration 56, loss = 0.04206322\n",
      "Iteration 57, loss = 0.03908955\n",
      "Iteration 58, loss = 0.04525123\n",
      "Iteration 59, loss = 0.03902016\n",
      "Iteration 60, loss = 0.03640040\n",
      "Iteration 61, loss = 0.02664762\n",
      "Iteration 62, loss = 0.04374379\n",
      "Iteration 63, loss = 0.02981702\n",
      "Iteration 64, loss = 0.03772316\n",
      "Iteration 65, loss = 0.02387300\n",
      "Iteration 66, loss = 0.03412439\n",
      "Iteration 67, loss = 0.02511624\n",
      "Iteration 68, loss = 0.02871630\n",
      "Iteration 69, loss = 0.02680133\n",
      "Iteration 70, loss = 0.03305442\n",
      "Iteration 71, loss = 0.02938807\n",
      "Iteration 72, loss = 0.02683736\n",
      "Iteration 73, loss = 0.02206507\n",
      "Iteration 74, loss = 0.02603176\n",
      "Iteration 75, loss = 0.03150726\n",
      "Iteration 76, loss = 0.02836421\n",
      "Iteration 77, loss = 0.04249080\n",
      "Iteration 78, loss = 0.03212675\n",
      "Iteration 79, loss = 0.02697811\n",
      "Iteration 80, loss = 0.03462300\n",
      "Iteration 81, loss = 0.04469939\n",
      "Iteration 82, loss = 0.03665403\n",
      "Iteration 83, loss = 0.03141793\n",
      "Iteration 84, loss = 0.04161247\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.41225074\n",
      "Iteration 2, loss = 1.30860516\n",
      "Iteration 3, loss = 1.23992847\n",
      "Iteration 4, loss = 1.15345954\n",
      "Iteration 5, loss = 1.06641097\n",
      "Iteration 6, loss = 0.95472740\n",
      "Iteration 7, loss = 0.86202954\n",
      "Iteration 8, loss = 0.77928054\n",
      "Iteration 9, loss = 0.68961088\n",
      "Iteration 10, loss = 0.62396077\n",
      "Iteration 11, loss = 0.55658565\n",
      "Iteration 12, loss = 0.49846367\n",
      "Iteration 13, loss = 0.44680441\n",
      "Iteration 14, loss = 0.39586726\n",
      "Iteration 15, loss = 0.33966206\n",
      "Iteration 16, loss = 0.31645697\n",
      "Iteration 17, loss = 0.28058131\n",
      "Iteration 18, loss = 0.24536649\n",
      "Iteration 19, loss = 0.21959249\n",
      "Iteration 20, loss = 0.19969420\n",
      "Iteration 21, loss = 0.17804430\n",
      "Iteration 22, loss = 0.17103282\n",
      "Iteration 23, loss = 0.14893066\n",
      "Iteration 24, loss = 0.13991942\n",
      "Iteration 25, loss = 0.12812664\n",
      "Iteration 26, loss = 0.12141271\n",
      "Iteration 27, loss = 0.10887211\n",
      "Iteration 28, loss = 0.09823184\n",
      "Iteration 29, loss = 0.09124724\n",
      "Iteration 30, loss = 0.09234544\n",
      "Iteration 31, loss = 0.08634379\n",
      "Iteration 32, loss = 0.07346252\n",
      "Iteration 33, loss = 0.08266407\n",
      "Iteration 34, loss = 0.07915061\n",
      "Iteration 35, loss = 0.07586049\n",
      "Iteration 36, loss = 0.06415191\n",
      "Iteration 37, loss = 0.06690621\n",
      "Iteration 38, loss = 0.05542134\n",
      "Iteration 39, loss = 0.05988659\n",
      "Iteration 40, loss = 0.06367561\n",
      "Iteration 41, loss = 0.05849755\n",
      "Iteration 42, loss = 0.05170095\n",
      "Iteration 43, loss = 0.05623877\n",
      "Iteration 44, loss = 0.05232488\n",
      "Iteration 45, loss = 0.04589985\n",
      "Iteration 46, loss = 0.05623140\n",
      "Iteration 47, loss = 0.05227109\n",
      "Iteration 48, loss = 0.04512267\n",
      "Iteration 49, loss = 0.04710799\n",
      "Iteration 50, loss = 0.03600076\n",
      "Iteration 51, loss = 0.04268490\n",
      "Iteration 52, loss = 0.05723476\n",
      "Iteration 53, loss = 0.03938418\n",
      "Iteration 54, loss = 0.04911387\n",
      "Iteration 55, loss = 0.04001703\n",
      "Iteration 56, loss = 0.03867632\n",
      "Iteration 57, loss = 0.04272751\n",
      "Iteration 58, loss = 0.03899385\n",
      "Iteration 59, loss = 0.04471565\n",
      "Iteration 60, loss = 0.03900510\n",
      "Iteration 61, loss = 0.04509424\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.39870296\n",
      "Iteration 2, loss = 1.31054860\n",
      "Iteration 3, loss = 1.23620382\n",
      "Iteration 4, loss = 1.15435736\n",
      "Iteration 5, loss = 1.05888260\n",
      "Iteration 6, loss = 0.96139092\n",
      "Iteration 7, loss = 0.86452959\n",
      "Iteration 8, loss = 0.78156820\n",
      "Iteration 9, loss = 0.70541216\n",
      "Iteration 10, loss = 0.62688110\n",
      "Iteration 11, loss = 0.55745391\n",
      "Iteration 12, loss = 0.51111695\n",
      "Iteration 13, loss = 0.45161462\n",
      "Iteration 14, loss = 0.38905540\n",
      "Iteration 15, loss = 0.34794518\n",
      "Iteration 16, loss = 0.31256772\n",
      "Iteration 17, loss = 0.28242194\n",
      "Iteration 18, loss = 0.24658651\n",
      "Iteration 19, loss = 0.21918499\n",
      "Iteration 20, loss = 0.20295016\n",
      "Iteration 21, loss = 0.18023926\n",
      "Iteration 22, loss = 0.16648965\n",
      "Iteration 23, loss = 0.14040071\n",
      "Iteration 24, loss = 0.13915041\n",
      "Iteration 25, loss = 0.12201227\n",
      "Iteration 26, loss = 0.11117877\n",
      "Iteration 27, loss = 0.11035504\n",
      "Iteration 28, loss = 0.09311719\n",
      "Iteration 29, loss = 0.09161193\n",
      "Iteration 30, loss = 0.08938060\n",
      "Iteration 31, loss = 0.07881629\n",
      "Iteration 32, loss = 0.07579309\n",
      "Iteration 33, loss = 0.06980739\n",
      "Iteration 34, loss = 0.07714477\n",
      "Iteration 35, loss = 0.08549005\n",
      "Iteration 36, loss = 0.05827450\n",
      "Iteration 37, loss = 0.07115274\n",
      "Iteration 38, loss = 0.06552070\n",
      "Iteration 39, loss = 0.05641758\n",
      "Iteration 40, loss = 0.05699054\n",
      "Iteration 41, loss = 0.05058777\n",
      "Iteration 42, loss = 0.05449703\n",
      "Iteration 43, loss = 0.05199218\n",
      "Iteration 44, loss = 0.04739072\n",
      "Iteration 45, loss = 0.06066298\n",
      "Iteration 46, loss = 0.05069896\n",
      "Iteration 47, loss = 0.04515882\n",
      "Iteration 48, loss = 0.04241612\n",
      "Iteration 49, loss = 0.05003948\n",
      "Iteration 50, loss = 0.03915240\n",
      "Iteration 51, loss = 0.03987295\n",
      "Iteration 52, loss = 0.03977459\n",
      "Iteration 53, loss = 0.03392960\n",
      "Iteration 54, loss = 0.04017153\n",
      "Iteration 55, loss = 0.03653103\n",
      "Iteration 56, loss = 0.04965442\n",
      "Iteration 57, loss = 0.03951933\n",
      "Iteration 58, loss = 0.04033388\n",
      "Iteration 59, loss = 0.04233578\n",
      "Iteration 60, loss = 0.03772379\n",
      "Iteration 61, loss = 0.02735346\n",
      "Iteration 62, loss = 0.04119247\n",
      "Iteration 63, loss = 0.02411038\n",
      "Iteration 64, loss = 0.03574872\n",
      "Iteration 65, loss = 0.03064395\n",
      "Iteration 66, loss = 0.04105120\n",
      "Iteration 67, loss = 0.02806038\n",
      "Iteration 68, loss = 0.03543756\n",
      "Iteration 69, loss = 0.04056204\n",
      "Iteration 70, loss = 0.03240900\n",
      "Iteration 71, loss = 0.03578061\n",
      "Iteration 72, loss = 0.01938401\n",
      "Iteration 73, loss = 0.02729144\n",
      "Iteration 74, loss = 0.02725342\n",
      "Iteration 75, loss = 0.03425855\n",
      "Iteration 76, loss = 0.03016230\n",
      "Iteration 77, loss = 0.02781858\n",
      "Iteration 78, loss = 0.03197573\n",
      "Iteration 79, loss = 0.02632252\n",
      "Iteration 80, loss = 0.02762287\n",
      "Iteration 81, loss = 0.03278609\n",
      "Iteration 82, loss = 0.02626568\n",
      "Iteration 83, loss = 0.03129022\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.41226415\n",
      "Iteration 2, loss = 1.30786712\n",
      "Iteration 3, loss = 1.23532187\n",
      "Iteration 4, loss = 1.14073056\n",
      "Iteration 5, loss = 1.05365888\n",
      "Iteration 6, loss = 0.94518778\n",
      "Iteration 7, loss = 0.85774031\n",
      "Iteration 8, loss = 0.76336181\n",
      "Iteration 9, loss = 0.67517481\n",
      "Iteration 10, loss = 0.61359321\n",
      "Iteration 11, loss = 0.54620489\n",
      "Iteration 12, loss = 0.48301930\n",
      "Iteration 13, loss = 0.42942959\n",
      "Iteration 14, loss = 0.37865682\n",
      "Iteration 15, loss = 0.34506911\n",
      "Iteration 16, loss = 0.29622592\n",
      "Iteration 17, loss = 0.27279950\n",
      "Iteration 18, loss = 0.24127819\n",
      "Iteration 19, loss = 0.21145124\n",
      "Iteration 20, loss = 0.18305222\n",
      "Iteration 21, loss = 0.17268318\n",
      "Iteration 22, loss = 0.15466117\n",
      "Iteration 23, loss = 0.13959165\n",
      "Iteration 24, loss = 0.12790451\n",
      "Iteration 25, loss = 0.11948488\n",
      "Iteration 26, loss = 0.11542014\n",
      "Iteration 27, loss = 0.10023305\n",
      "Iteration 28, loss = 0.09461938\n",
      "Iteration 29, loss = 0.09505543\n",
      "Iteration 30, loss = 0.07869098\n",
      "Iteration 31, loss = 0.08342095\n",
      "Iteration 32, loss = 0.06858080\n",
      "Iteration 33, loss = 0.07164682\n",
      "Iteration 34, loss = 0.06889630\n",
      "Iteration 35, loss = 0.06207408\n",
      "Iteration 36, loss = 0.06852593\n",
      "Iteration 37, loss = 0.06540851\n",
      "Iteration 38, loss = 0.05600558\n",
      "Iteration 39, loss = 0.05009536\n",
      "Iteration 40, loss = 0.05569798\n",
      "Iteration 41, loss = 0.04867341\n",
      "Iteration 42, loss = 0.04805910\n",
      "Iteration 43, loss = 0.05621606\n",
      "Iteration 44, loss = 0.05986553\n",
      "Iteration 45, loss = 0.04110623\n",
      "Iteration 46, loss = 0.05120013\n",
      "Iteration 47, loss = 0.04607688\n",
      "Iteration 48, loss = 0.04125223\n",
      "Iteration 49, loss = 0.04741335\n",
      "Iteration 50, loss = 0.04287680\n",
      "Iteration 51, loss = 0.04314508\n",
      "Iteration 52, loss = 0.04496625\n",
      "Iteration 53, loss = 0.04709848\n",
      "Iteration 54, loss = 0.04923154\n",
      "Iteration 55, loss = 0.04690668\n",
      "Iteration 56, loss = 0.04622717\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.349444352317947,\n",
       " 0.012940846656138935,\n",
       " 0.9944360547467739,\n",
       " 0.0024133791990319704)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redes_neuronales()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
